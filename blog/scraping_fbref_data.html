<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<title>Pranav's Blog</title>
	<meta name="description" content="This articles aims at scraping dynamically generated data/tables from a website, when our veteran rvest just can't work on his own.">
	<meta name="author" content="Pranav Nagarajan">
	<meta name="viewport" content="width=device-width, initial-scale=1">


	<meta property="og:locale" content="en_US">
	<meta property="og:type" content="article">
	<meta property="og:title" content="Scraping  Data from FBref using RSelenium and Docker Desktop (A Complete Package) | @npranav10">
	<meta property="og:description" content="This articles aims at scraping dynamically generated data/tables from a website, when our veteran rvest just can't work on his own.">
	<meta property="og:url" content="https://npranav10.github.io/blog/scraping_fbref_data.html">
	<meta property="og:site_name" content="Pranav's Blog">
	<meta property="article:publisher" content="https://npranav10.github.io/">
	<meta property="article:section" content="Main">
	<meta property="article:published_time" content="2020-02-21T06:30:00+00:00">
	<meta property="og:image" content="https://pbs.twimg.com/profile_images/962426011626409984/PPzi_xEL_400x400.jpg">
	<meta property="og:image:secure_url" content="https://pbs.twimg.com/profile_images/962426011626409984/PPzi_xEL_400x400.jpg">
	<meta property="og:image:width" content="1230">
	<meta property="og:image:height" content="860">
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:description" content="This articles aims at scraping dynamically generated data/tables from a website, when our veteran rvest just can't work on his own.">
	<meta name="twitter:title" content="Scraping  Data from FBref using RSelenium and Docker Desktop (A Complete Package) | @npranav10">
	<meta name="twitter:site" content="@npranav10">
	<meta name="twitter:image" content="https://pbs.twimg.com/profile_images/962426011626409984/PPzi_xEL_400x400.jpg">
	<meta name="twitter:creator" content="@npranav10">
	
	
	
	<link href="https://fonts.googleapis.com/css?family=sans-serif:400,300,600" rel="stylesheet" type="text/css">
	<link rel="stylesheet" href="../blog/css/normalize.css">
	<link rel="stylesheet" href="../blog/css/skeleton.css">
	<link href="../css/font-awesome.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<style>
		img {
		/* margin: 100px; */
		transition: transform 0.25s ease;display: block;
		cursor: zoom-in;border: 1px solid black !important;margin: 20px auto;text-align: center;
		}
		img:hover {
		transform: scale(1.7);
		cursor: zoom-out;
		}
		.supporting-img {
		border: 0;width: 60%;
		}
		.nav {
			background: #0F2E3D;
			color: #FFFFFF;
		}
		.nav a {
			color: inherit;
			text-decoration: inherit;
		}
		.article-card {
			padding:10px;
			border-radius:10px;
			box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
			margin:30px auto;
		}
		.thumb img {
			  display: block;
			  margin-left: auto;
			  margin-right: auto;width:100%;
		}
		.title {
			font-weight: bold; text-decoration: underline;
		}
		a, a:hover {
			color:inherit;
		}
		.article-card {
			-webkit-transition: background 0.5s; /* Safari */
		  transition: background 0.5s;
		}
		.article-card:hover {
			background: #f1f6ff;
		}
		.header {
		position: fixed;
		top: 0;
		z-index: 1;
		width: 100%;
		height: 50px;
		background-color: #0F2E3D;
		}

		.header h5 {
		text-align: center;padding-left: 5%;padding-top: 10px;font-size: 30;
		}

		.progress-container {
		width: 100%;
		height: 8px;
		background:rgb(204, 52, 25);
		}
		.progress-bar {
		height: 8px;
		background: rgb(130, 235, 130);
		width: 0%;
		}
		.content{
			padding: 50px 50px 50px 50px;margin: 50px auto 0 auto;width: 65% !important;
			font-family:"PT Sans", Helvetica, Arial, sans-serif !important;
			/* letter-spacing: 0.04rem !important; */
		}
		.code {
		padding: 10px 10px 10px 10px;
		margin: 10px 10px 10px 10px;
		font-family: monospace;
		font-size: large !important;
		background-color: #f1f1f1;
		word-wrap: anywhere;
		}
		body{
			font-family:"PT Sans", Helvetica, Arial, sans-serif !important; color: #515151;
		}
		.floatleft{
			width: 60%;float: left;
		}
		.floatright{
			width: 40%;float: right;
		}
		code {
	padding: .2rem .5rem;
	margin: 0 .2rem;
	font-size: 90%;
	white-space: nowrap;
	background: #F1F1F1;
	border: 1px solid #E1E1E1;
	border-radius: 4px;
	white-space: inherit;
}
	</style>

</head>

<body>
	<!-- Header  -->
	<div class="nav container u-full-width u-max-full-width">
		<div class="header">
			<div class="row">
				<div class="column" style="height: 50px;">
					<!-- <a href="https://npranav10.github.io" target="_blank"><h5 style="margin-top:1.2rem;">Check out my Portfolio</h5></a> -->
					<h5>Scraping  Data from FBref using RSelenium and Docker Desktop (A Complete Package)</h5>
				</div>
			</div>
			<div class="progress-container">
				<div class="progress-bar" id="myBar" style="width: 0%;"></div>
			  </div>
		</div>
	</div>

	<!-- Body -->
	<div class="content">
	<!-- >Disclaimer -->

	<div class="code">
		If you are planning to make use of the following procedure to scrap data from a Sports Reference page, please go through the
		 <a href="#disclaimer" style="color: blue;">Disclaimer</a><br>
	</div>

	<!-- >Pre-Match Conference -->
	
	<div class="row"> 			
		<h4 class="title">Pre-Match Conference:</h4>
		<p>
			</p><ul style="list-style-type:square;">
			<li>The idea to write this article was always in my mind when I managed to scrap Fbref data for myself.</li>
			<li>But only when I shared this info with my friends
			who are working in the same field, I realised that a helper was not available online and that they were waiting for someone to do this daunting task.</li> 
			<li>This procedure is not only applicable for Fbref or any SRef in that case but also for data in any website which is rendered dynamically.</li>
			<div class="code">
				If you would like to directly skip to the procedure you can jump to <a href="#gettingstarted" style="color: blue;">Getting Started</a><br>
				Or If you just need the code you can travel to <a href="#code" style="color: blue;">Final Code</a>
			</div>
			</ul>
		<p></p>
	</div>	

	<!-- >What we normally used to do ? -->

	<div class="row"> 			
		<h4 class="title">What we normally used to do ?</h4>

				The traditional data scraping through rvest() is as follows:
				<ol type="1">
					<li>
						Grab the page URL and table-id
						<center>
								<img class="supporting-img" src="./trad_tableid.png"> 
						</center>	
					</li>
					<li>
						And then (har)rvest it
						<div class="code">
							require("dplyr")<br><br>
							stats_passing_squads = xml2::read_html("https://fbref.com/en/comps/22/passing/Major-League-Soccer-Stats") %&gt;%
							rvest::html_nodes("#stats_passing_squads") %&gt;%
							rvest::html_table()
							<br><br>
							stats_passing_squads = stats_passing_squads[[1]]
						</div>
						<center>
								<img class="supporting-img" src="./scrap_trad_way.png" style="width: 50%;">  
						</center>	
					</li>
				</ol>
	</div>	

	<!-- whats special in fbref tables -->
	<div class="row">
			<h4 class="title">What's special with FBref tables and Why just using rvest won't help?</h4>

			<div class="row">	
				<ul style="  list-style-type: disc;">
					<li>
						The site is structured in such a way that all tables except the first (even the 1st sometimes), are rendered dynamically. 
						<b>I managed to capture the way the tables load.</b>
					</li>
					<div class="row">	
				<img class="supporting-img" style="width: 40%;" src="./fbref_dyn-tab-gen.png">
	</div><li>
						Notice how the 1st table loads quickly and how the reminder are taking their time to load.
					</li>
					<li>
						I have annotated the tableids on the tables for the sake of clarity.
					</li>

				</ul>	
			</div>
			
	</div>
	

	<!-- failed scrap -->
	<div class="row">
		<p>
			So when we try to scrap such tables, this happens :
		</p>
			<img class="supporting-img" src="./empty_list.PNG" style="width: 80% !important;">
	</div>

	<hr>
	<!-- What next -->
	<div class="row"> 
		<h4 class="title">What Next? :(</h4>
		<p>
			The issue is we can see the tables and their ids in naked eye but can't <strike>put it on paper</strike> use them in R Studio. So we are going to use <strong>RSelenium</strong> which helps us in this matter.
		</p>
		How does RSelenium help us?
		<ul style="  list-style-type: disc;">
			<li>
				Selenium is an automation tool. RSelenium is a package which helps us to achieve automation in R.
			</li>
			<li>
				By using RSelenium, we fool the website as if a browser has walked through the website completely and collected every bit of info on its way. The server will think the selenium driver to be just another human wandering the page.
			</li>
			<li>
				In other words, it's similar to copying the contents of whole page and saving it in a word document for future purposes. Get it right? It's as simple as this.
			</li>

		</ul>	
		<strong><em>Let's get started<br><br></em></strong>

			But Before getting started, there is a catch.<br><br>
			We can't just enter the url in a browser and ask "RSelenium" to do its job. 
			We need to install Selenium in our computer, so that RSelenium can communicate with real Selenium. 
			And for that we will use the concept of containers (Docker) rathering than using executables which are tiresome and has different procedure for different OSes.
			<br>
			<strong>Hold your breath. All these are steep leaning curves and rather one-time investments.</strong>
			<br>
	</div>
	<hr id="gettingstarted">

	<!-- Getting Started -->
	<div class="row"> 
		<h4 class="title"><u>Getting Started:</u></h4>
		<ul style="  list-style-type: disc;">
			<li>
				Install <a href="https://www.docker.com/products/docker-desktop" target="_blank" style="color: blue;">Docker for Desktop</a>
			</li>	
			<li>
				Install RSelenium in RStudio as follows:
				<code>install.packages("RSelenium")</code>
			</li>	
			<li>
				Execute the following code in RStudio's Console unless stated otherwise.
			</li>			
		</ul>
		Now with all dependancies installed, let's get into the action <br>
		<ol type="1">
			<li>
				Start <code>Docker for Desktop</code> in your OS. (It really takes time to load. Watch Haaland's goals meanwhile.)
			</li>	
			<li>
				Run the following code in RStudio's <strong>Terminal (not Console)</strong>:
			<code>docker run -d -p 4445:4444 selenium/standalone-chrome</code><br>
			The de facto way of knowing the success status is by running <code>docker ps</code> on which you will get the container id of selenium as output.<br>
			If Terminal is not visible in your panes, then use <code>ALT + SHIFT + R</code> to open it.<br>
			</li>
			<div class="row">
				<center><img src="terminal.png" style="width: 30%; display: block;">
				</center>
				</div>
			<li>
				Remember the anonymous browser that I mentioned earlier? We are about to invoke him now:
				<!-- <div class="row"> -->
				<div class="row">
				<div class="code ">
					remDr &lt;- RSelenium::remoteDriver(remoteServerAddr = "localhost",port = 4445L,browserName = "chrome") <br>
					remDr$open() <br><br>
					remDr$navigate("https://fbref.com/en/comps/22/passing/Major-League-Soccer-Stats") <br>
					remDr$screenshot(display = TRUE)
				</div>
				</div>
				<div class="row">
				<center><img src="remDr_screenshot.PNG" style="width: 40%; display: block;">
					<p>
						Magnifique !
					</p></center>
				</div>
			<!-- </div> -->
			</li>
			<li>
				The rest of the code involves rvest where we will be dumping the html source code into read_html()<br>
				<div class="code">
					require("dplyr") <br>
					stats_passing_squads = xml2::read_html(remDr$getPageSource()[[1]]) %&gt;% rvest::html_nodes("#stats_passing_squads") %&gt;% rvest::html_table() <br>
					stats_passing_squads = stats_passing_squads[[1]] 
				</div>
				<center><img src="scrape_success.PNG" style="width: 80%; display: block;">
					<p>
						Großartig !
					</p></center>
			</li>
		</ol>
		
	</div>
	<hr id="code">
	<!-- Scripting (Automation) -->
	<div class="row">
		You - "So Pranav. You are asking me to follow the above steps everytime?" <br>
		Me - " No not absolutely. Hence comes the most awaited part."<br><br>
		We can create a function to automate all these steps (including running Docker from Console). Here's how I do it.<br>
		<h4 class="title"><u>FBref data at one click of a button</u></h4>
		<div class="code">
			getFBrefStats = function(url,id){
				<br>
				require(RSelenium) <br>
				require(dplyr) <br>
				<br>
				# For some unspecified reason we are starting and stopping the docker container initailly.<br>
				# Similar to heating the bike's engine before shifting the gears.<br>
				system("docker run -d -p 4445:4444 selenium/standalone-chrome")<br>
				t = system("docker ps",intern=TRUE)<br>
				if(is.na(as.character(strsplit(t[2],split = " ")[[1]][1]))==FALSE) <br>
				{<br>
				  system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))<br>
				}<br>
				<br>
				# To avoid starting docker in Terminal  <br>
				<br>
				system("docker run -d -p 4445:4444 selenium/standalone-chrome") <br>
				Sys.sleep(3) <br>
				remDr &lt;- RSelenium::remoteDriver(remoteServerAddr = "localhost",
												 port = 4445L,
												 browserName = "chrome")
												 <br>
				# Automating the scraping initiation considering that Page navigation might crash sometimes in <br>
				# R Selenuium and we have to start the process again. Good to see that this while() logic  <br>
				# works perfectly <br>
				<br>
				while (TRUE) { <br>
				  tryCatch({ <br>
					#Entering our URL gets the browser to navigate to the page <br>
					remDr$open() <br>
					remDr$navigate(as.character(url))  <br>
				  }, error = function(e) { <br>
					remDr$close() <br>
					Sys.sleep(2) <br>
					print("slept 2 seconds") <br>
					next <br>
				  }, finally = { <br>
					#remDr$screenshot(display = TRUE) #This will take a screenshot and display it in the RStudio viewer <br>
					break <br>
				  }) <br>
				} <br>
				<br>
				# Scraping required stats <br>
				<br>
				data &lt;- xml2::read_html(remDr$getPageSource()[[1]]) %&gt;% <br>
				  rvest::html_nodes(id) %&gt;% <br>
				  rvest::html_table() <br>
				data = data[[1]] <br>
				<br>
				remDr$close() <br>
				remove(remDr) <br>
				# Automating the following steps: <br>
				# 1. run "docker ps" in Terminal and get the container ID from the output <br>
				# 2. now run "docker stop container_id" e.g. docker stop f59930f56e38 <br>
				<br>
				t = system("docker ps",intern=TRUE) <br>
				system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep="")) <br>
				<br>
				return(data) <br>
			}
		</div>
		<u>Test Drive:</u> <br>
		All you have to do is start "Docker for Desktop" and call our function in RStudio. 

		<div class="code">
			bundesliga_players_fbref_shooting = getFBrefStats("https://fbref.com/en/comps/20/shooting/Bundesliga-Stats","#stats_shooting") <br>
			head(bundesliga_players_fbref_shooting)
		</div>
		<img src="bundesliga_players_fbref_shooting.PNG" width="100%">
		<div class="floatleft">
		Full Time!<br> <img src="https://cdn4.iconfinder.com/data/icons/mixed-6/100/mixed-27-512.png" style="float: left;" width="10%">
		</div>

	</div>


	<!-- >Post-Match Conference -->

	<div class="row"> 			
		<h4 class="title">Post-Match Conference:</h4>
		Where you can make full use of such automation?
		<p>
			</p><ul style="list-style-type:square;">
			<li>Building a shiny app which makes something out of fbref data.</li>
			<li>The only catch is you can't run Docker in shinyapps.io . Instead you can host your shiny app in a cloud such as AWS or Azure and 
				then run Docker for Desktop 24 hrs * 365 days or at least until breach your free-tier limit.
			</li></ul> 
			 If you wish to see me provide such an example for our community, just drop a message at <a href="https://twitter.com/npranav10" target="blank">@npranav10</a>
				<a href="https://twitter.com/npranav10" class="fa fa-twitter" target="blank" style=" color: #1da1f2;
				font-size: 30px;
				width: 50px;
				text-align: center;
				text-decoration: none;
				margin: 5px 2px;"></a>
		<p></p>
		<p>Also a shout-out to <a href="https://twitter.com/etmckinley" target="blank">Eliot McKinley</a> for encouraging me to write this article.</p>
	</div>	
	<div class="row">
		<h4 class="title">Reference:</h4>
		<a href="https://callumgwtaylor.github.io/blog/2018/02/01/using-rselenium-and-docker-to-webscrape-in-r-using-the-who-snake-database" target="blank" >
			Callum Taylor : Using RSelenium and Docker To Webscrape In R - Using The WHO Snake Database</a> <br>
		Apart from borrowing few snippets from the above piece, I did manage to bring in some automation into the procedure to gather data.<br>
		If you feel, there is also another way to achieve this, don't hesitate to contact me.
	</div>
	<hr>
	<div class="row" id="disclaimer">
		<h4 class="title" style="color: red;">Disclaimer:</h4>
		<p>		Sports Reference LLC says : "Except as specifically provided in this paragraph, you agree not to use or launch any automated system, 
			including without limitation, robots, spiders, offline readers, or like devices, that accesses the Site in a manner which sends more request 
			messages to the Site server in any given period of time than a typical human would normally produce in the same period by using a conventional 
			on-line Web browser to read, view, and submit materials."
		</p>
		<ul style="list-style-type: disc;">
			<li>
				I would like to re-iterate that in this procedure, the time taken to fetch data from any SRef website is same as someone copying it from a browser,
				because of the way RSelenium works.
			</li>
			<li>
				Also the function to get such data was made to help people who prefer to get the data in click of a button rather than:
				<ol>
					<li>
						Opening a page
					</li>
					<li>
						Scrolling through for a table
					</li>
					<li>
						Clicking "Share & More"
					</li>
					<li>
						Choosing "Get Table as CSV" and Downloading it.
					</li>
					<li>
						And then loading csv file in R
					</li>
				</ol>
			</li>
			<li>
				If you are a Sports Reference official and you would like me to avoid using FBref as an example, kindly message me.
			</li>
		</ul>
	</div>

<!-- div class="content" ends -->
	</div> 



	  
<script>
	// When the user scrolls the page, execute myFunction
	window.onscroll = function() {myFunction()};
	
	function myFunction() {
	  var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
	  var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
	  var scrolled = (winScroll / height) * 100;
	  document.getElementById("myBar").style.width = scrolled + "%";
	} 
</script>

</body></html>